# these are the default settings that MARS will use if you don't override them

# relative paths to the models used for detection and pose estimation.
black_detector_front: models/detection/MARS_front_detection_black.pb
white_detector_front: models/detection/MARS_front_detection_white.pb
black_detector_top: models/detection/MARS_top_detection_black.pb
white_detector_top: models/detection/MARS_top_detection_white.pb
front_pose_model: models/pose/MARS_front_pose.pb
top_pose_model: models/pose/MARS_top_pose.pb
classifier_model: models/classifier

# information about your videos. Set framerate=-1 to get the framerate value from the video file.
pixels_per_cm: 1
framerate: 20
num_mice: 1

# which stages of analysis to run- pose estimation, feature extraction, and behavior classification
# feature extraction is a required step if you wish to do behavior classification, and
# pose estimation is a required step if you wish to do feature extraction.
doPose: False
doFeats: True
doActions: True

# setting multichannelAnotations to true will output predictions of each binary classifier to a separate annotation
# channel, allowing frames to potentially be labeled positive for more than one behavior.
multichannelAnnotations: True

# enable to have MARS save a video of the resulting pose estimation
doVideo: False

# overwrite any existing pose/feature/annotation files for a given video
doOverwrite: False

# cameras available for this experiment. This will determine which detection/pose model is used.
# note, feature extraction and action classification require a top-view camera, and ignore the front-view camera:
hasTopCamera: True
hasFrontCamera: False

# enable background-subtraction prior to mouse detection/pose estimation. This is an experimental feature, it might
# help MARS to do a better job in arenas it hasn't seen before?
bgSubtract: False

# import bounding boxes produced by a different system, and use these instead of MARS's detection step.
# If set to true, bboxType tells MARS how to unpack the user-provided boxes.
# this feature is still in development.
useExistingBBoxes: False
bboxType:

# output verbosity
verbose: 1

# max number of frames to process from a video
max_frames: 9999999
